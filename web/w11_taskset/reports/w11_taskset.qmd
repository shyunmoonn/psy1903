---
title: "Week 11 Taskset"
author: "Shyun Moon"
date: "11/12/2025"
format: html
---

#### Concept Check 
+ Q1: What does source("scripts/score_questionnaire.R") enable in your workflow?
    + It creates a function that can be used to turn JSON strings       from the questionnaire row and return a single score.
  
+ Q2: Why is modularizing your code into multiple scripts considered a best practice?
    + It allows us to create reusable and clear functions in            separate scripts, where each script should focus on one           logical task. 
  
+ Q3: What information does traceback() provide after an error?
    + It provides the sequence of calls that lead to the error.
  
+ Q4: When you read multiple .csv files into R, how can using str() or names() before combining them help you prevent or debug errors later in your workflow?
    + You can check that all files have the same column names and       data types because R matches columns by name when combining       files.
    
+ Q5: When you run source("scripts/process_participant.R") inside your Quarto document, nothing prints in the Console.
How can you check whether your function actually loaded correctly into your environment, and why is this step important before calling it in later code?
    + I can check whether my function loaded by running ls() or objects() in my console and seeing that it is in the environment that I am working in. This step is important because if it is not in the environment, the code won't know what I'm referencing when I call it later. 


#### Workflow 
First, each participant's data is imported with the function `process_participant`. Next, missing reaction times are recovered. Once RTs are recovered, accuracy is converted to logical TRUE/FALSE values and the dataset is filtered to include only correct trials, which allows for a calculation of mean RT within the 300â€“900 ms window. After behavior is summarized, the ESQ score is calculated. Finally, I combine the behavioral summary and the ESQ score into a one-row participant summary. Using `lapply()` and `do.call(rbind, ...)`, I merge all participant summaries into a single study-level data frame that contains one row per subject. I handled missing data by recovering RTs and treating true missing values as 'NA'. 


#### Load functions
```{r}
source("../scripts/score_questionnaire.R")
source("../scripts/summarize_behavior.R")
source("../scripts/process_participant.R")
source("../scripts/compute_rt_if_missing.R")

```


#### Find Files 
```{r Q8}
file_list <- list.files(
  here::here("data", "raw"), 
  pattern = "^est-experiment-.*\\.csv$",
  full.names = TRUE
)
```

#### Apply participant processor 
```{r}
participant_rows <- lapply(file_list, process_participant)

```
+ We filter RTs to 300-900 ms to prevent outliers (e.g. unnaturally quick or slow reaction times). 

#### Combine into one study-level data frame
```{r}
study_level <- do.call(rbind, participant_rows)

```

#### Visualization
```{r}
knitr::kable(study_level)
```

The mean reaction time across the study is `r mean(study_level$mean_rt_correct, na.rm = TRUE)` ms, and the mean accuracy is `r mean(study_level$mean_accuracy, na.rm = TRUE)`. 





